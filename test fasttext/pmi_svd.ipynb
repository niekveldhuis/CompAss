{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create file with all words (lemmas) of a single text listed in a row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output', 'corpus']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Project(s):  epsd2/admin/ur3\n"
     ]
    }
   ],
   "source": [
    "projects = input('Project(s): ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://build-oracc.museum.upenn.edu/json/epsd2-admin-ur3.zip as jsonzip/epsd2-admin-ur3.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59564eb01d79486a90d2dd2a9b1ee4df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=577877494.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['epsd2/admin/ur3']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = format_project_list(projects)\n",
    "oracc_download(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        field = ''\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        if \"type\" in JSONobject and JSONobject[\"type\"] == \"field-start\":\n",
    "            field = JSONobject[\"subtype\"]\n",
    "        if \"f\" in JSONobject and not field in ['sg', 'pr']: # skip the fields \"sign\" and \"pronunciation\"\n",
    "                                # in lexical texts\n",
    "            if JSONobject[\"f\"][\"lang\"][:3] == \"sux\": #only Sumerian and Emesal\n",
    "                word = JSONobject[\"f\"]\n",
    "                if \"cf\" in word:\n",
    "                    if 'pos' in word:  #for some reason some words appear without pos. Provisionally treated as Noun\n",
    "                        lemm = word[\"cf\"] + '[' + word[\"gw\"] + \"]\" + word[\"pos\"]\n",
    "                    else:\n",
    "                        lemm = word[\"cf\"] + '[' + word[\"gw\"] + \"]N\"\n",
    "                    lemm = lemm.replace(' ', '-') # remove commas and spaces from lemm\n",
    "                    lemm = lemm.replace(',', '')\n",
    "                else:\n",
    "                    lemm = \"_\" # if word is unlemmatized enter a place holder\n",
    "                l.append(lemm)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "744249b6704244179a1b6098359e5c0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=71712.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsd2/admin/ur3/P143238 is not available or not complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_ = []\n",
    "ids_ = []\n",
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc = project):                            #iterate over the file names\n",
    "        l = []\n",
    "        id_no = filename[-13:-5]\n",
    "        if id_no in ids_ and not \"X\" in id_no: # Check if P/Q number is already in there\n",
    "            continue        # a text may appear in multiple projects\n",
    "        id_text = project + id_no # id_text is, for instance, blms/P414332\n",
    "        ids_.append(id_text)\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            #lemm_.append(f\"\\n{id_text}\")     # new text starts on new line with text_id\n",
    "            parsejson(data_json)\n",
    "            lemm_.append(l)\n",
    "        except:\n",
    "            print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in the list of lists lemm_, which holds the individual texts. Each individual text is represented by a list of lemmas, with the lemmas in the original order. Currently, breaks etc. are not represented. Secondly, the list ids_ holds all the text IDs; the list ids_ has the same order as the list of lists lemm_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is directly taken from the blog post \"simple word vectors with co-occurrence pmi and svd\" by Alex Klibisz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Klibisz is working with *titles* he compares all possible bigrams and does not define a moving window. That probably needs to change. Aleksi creates windows with the following code:\n",
    "```python\n",
    "wz = self.windowsize - 1\n",
    "zip(*[text[i:] for i in range(1+wz*2)])\n",
    "```\n",
    "This will zip multiple versions of the same text - the first starts at word 0, the next at word 1, etc. Since the window is symmetric (counting forward and backward) take it twice .\n",
    "An alternative method is to use NLTK ngrams, which will create windows.\n",
    "```python\n",
    "from nltk import ngrams\n",
    "n = 1+wz*2\n",
    "windows = ngrams(text, n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29.824 seconds (0.00042 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2a. Compute unigram and bigram counts.\n",
    "# A unigram is a single word (x). A bigram is a pair of words (x,y).\n",
    "# Bigrams are counted for any two terms occurring in the same title.\n",
    "# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n",
    "# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\n",
    "t0 = time()\n",
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "for text in lemm_:\n",
    "    cx.update(text)\n",
    "\n",
    "    # Count all pairs of words, even duplicate pairs.\n",
    "    windows = ngrams(text, 7) # 7 is window length - needs to be changeable\n",
    "    for w in windows: # this creates the windows\n",
    "        z = [tuple(l) for l in map(sorted, combinations(w, 2))]\n",
    "        cxy.update(z)\n",
    "\n",
    "#     # Alternative: count only 2-grams.\n",
    "#     for x, y in zip(text[:-1], text[1:]):\n",
    "#         cxy[(x, y)] += 1\n",
    "\n",
    "#     # Alternative: count all pairs of words, but don't double count.\n",
    "#     for x, y in set(map(tuple, map(sorted, combinations(text, 2)))):\n",
    "#         cxy[(x,y)] += 1\n",
    "\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / len(lemm_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21536 tokens before\n",
      "0.012 seconds (0.00000 / iter)\n",
      "7167 tokens after\n",
      "Most common: [('2[00]PN', 30472), ('kišib[seal]N', 27779), ('ninda[bread]N', 27081), ('i[oil]N', 22750), ('gud[ox]N', 22281), ('ŋuruš[male]N', 21149), ('hulu[bad]V/i', 20949), ('dubsar[scribe]N', 20174), ('us[follow]V/t', 19627), ('šu[hand]N', 17869), ('niga[fattened]V/i', 17528), ('ugula[overseer]N', 16442), ('sila[lamb]N', 16036), ('ŋiri[foot]N', 16020), ('naŋa[potash]N', 15631), ('šum[garlic]N', 15391), ('šag[heart]N', 15094), ('teŋ[near]V/i', 14838), ('mana[unit]N', 14377), ('maš[goat]N', 14110), ('ašag[field]N', 14024), ('iku[unit]N', 13330), ('lal[small]V/i', 11255), ('dab[seize]V/t', 11003), ('du[go]V/i', 10747)]\n"
     ]
    }
   ],
   "source": [
    "# 2b. Remove frequent and infrequent unigrams.\n",
    "# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n",
    "# very frequently or infrequently. This decreases the vocab size substantially.\n",
    "print('%d tokens before' % len(cx))\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "min_count = 4\n",
    "max_count = sx * .01\n",
    "for x in list(cx.keys()):\n",
    "    if cx[x] < min_count or cx[x] > max_count:\n",
    "        del cx[x]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))\n",
    "print('%d tokens after' % len(cx))\n",
    "print('Most common:', cx.most_common()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.342 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2c. Remove frequent and infrequent bigrams.\n",
    "# Any bigram containing a unigram that was removed must now be removed.\n",
    "t0 = time()\n",
    "for x, y in list(cxy.keys()):\n",
    "    if x not in cx or y not in cx:\n",
    "        del cxy[(x, y)]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.003 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 3. Build unigram <-> index lookup.\n",
    "t0 = time()\n",
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 4. Sum unigram and bigram counts for computing probabilities.\n",
    "# i.e. p(x) = count(x) / sum(all counts).\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "sxy = sum(cxy.values())\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / (len(cx) + len(cxy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.970 seconds (0.00000 / iter)\n",
      "621175 non-zero elements\n",
      "Sample PMI values\n",
      " [(('en.lil₂.da.mah.di[00]PN', 'šul.gi.he₂.ti[00]PN'), 12.10400060590684),\n",
      " (('gubar[nape]N', 'gumur[spine]N'), 12.043375984090407),\n",
      " (('Šu.diš[00]PN', 'Šu.diš[00]PN'), 11.755693911638625),\n",
      " (('mesbabbar[tree]N', 'mesbabbar[tree]N'), 11.755693911638625),\n",
      " (('E.pe.eš[00]PN', 'Na.am.tu.ra[00]PN'), 11.731596360059564),\n",
      " (('en[lord]N', 'šaŋar[oppressed]V/i'), 11.706903747469193),\n",
      " (('Geš.gi.tur.tur[00]FN', 'šeš.da.edin.na[00]DN'), 11.668682534648996),\n",
      " (('anki[universe]N', 'suh[confuse]V/t'), 11.622162519014102),\n",
      " (('nin.gir₂.su.gu₂.gal[00]FN', 'nin.gir₂.su.ka.bi₂.du₁₁[00]PN'),\n",
      "  11.57337235484467),\n",
      " (('E₂.lu₂.lagar[00]PN', 'Nin.ba.tuku.šem₅.du₁₀.gal[00]PN'),\n",
      "  11.483760196154984)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n",
    "# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n",
    "# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n",
    "# The probabilities are computed on the fly using the sums from above.\n",
    "t0 = time()\n",
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in cxy.items():\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    data.append(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)))\n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PMI = csc_matrix((data, (rows, cols)))\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\n",
    "print('%d non-zero elements' % PMI.count_nonzero())\n",
    "print('Sample PMI values\\n', pformat(pmi_samples.most_common()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.187 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. Factorize the PMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n",
    "# This part replaces the stochastic gradient descent used by Word2vec\n",
    "# and other related neural network formulations. We pick an arbitrary vector size k=20.\n",
    "t0 = time()\n",
    "U, _, _ = svds(PMI, k=20)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n",
    "# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\n",
    "t0 = time()\n",
    "norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\n",
    "U /= np.maximum(norms, 1e-7)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suhur[carp]N, 78\n",
      " (mušenturtur[bird]N, 0.917) (saŋkur[fish]N, 0.938) (tun[container]N, 0.935) (saŋkešed[fish]N, 0.922) (nunuz[egg]N, 0.907) \n",
      "----------\n",
      "gigir[chariot]N, 299\n",
      " (šudu[equipped]AJ, 0.746) (šegin[glue]N, 0.739) (dur[door-socket]N, 0.731) (bariga[unit]N, 0.692) (kaŋeškarak[table]N, 0.687) \n",
      "----------\n",
      "šah[pig]N, 486\n",
      " (zeda[piglet]N, 0.919) (saŋkur[fish]N, 0.916) (pešgi[rodent]N, 0.931) (Ša₃.gul.lum[00]PN, 0.907) (Šu.bar[00]PN, 0.907) \n",
      "----------\n",
      "Inanak[1]DN, 394\n",
      " (Gud-sisa[1]MN, 0.827) (NE.NE.ŋar[1]MN, 0.802) (Gangane[1]MN, 0.763) (Kin-Inana[1]MN, 0.750) (Apinduʾa[1]MN, 0.738) \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 8. Show some nearest neighbor samples as a sanity-check.\n",
    "# The format is <unigram> <count>: (<neighbor unigram>, <similarity>), ...\n",
    "# From this we can see that the relationships make sense.\n",
    "k = 5\n",
    "for x in ['suhur[carp]N', 'gigir[chariot]N', 'šah[pig]N', 'Inanak[1]DN']:\n",
    "    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n",
    "    s = ''\n",
    "    # Compile the list of nearest neighbor descriptions.\n",
    "    # Argpartition is faster than argsort and meets our needs.\n",
    "    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:\n",
    "        if i2x[i] == x: continue\n",
    "        xy = tuple(sorted((x, i2x[i])))\n",
    "        s += '(%s, %.3lf) ' % (i2x[i], dd[i])\n",
    "    print('%s, %d\\n %s' % (x, cx[x], s))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"suhur[carp]N\" in x2i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "for text in lemm_: \n",
    "    c.update(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Counter()\n",
    "text = lemm_[-2]\n",
    "z  = [tuple(l) for l in map(sorted, combinations(text, 2))]\n",
    "c.update(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(map(sorted, combinations(text, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wz = 2\n",
    "w = zip(*[text[i:] for i in range(1+wz*2)])\n",
    "list(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = lemm_[-2]\n",
    "[text[i:] for i in range(3)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(range(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len([text[i:] for i in range(1+wz*2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import ngrams\n",
    "\n",
    "sentence = 'this is a foo bar sentences and i want to ngramize it'\n",
    "\n",
    "n = 1+wz*2\n",
    "window = ngrams(text, n)\n",
    "\n",
    "for grams in window:\n",
    "  print(grams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
