{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Ur III Commodities and Actors: Word Embeddings\n",
    "\n",
    "The basic idea behind word embeddings is that word meaning is determined by the contexts in which the word is found. Or, in the famous quote by the linguist [J.R. Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth): \"You shall know a word by the company it keeps.\" Each unique word (or lemma) in a corpus is assigned a vector in such a way that words attested in similar contexts receive similar vectors. As a result, vectors that represent words with similar meanings become neighbors: they are relatively close in the vector space.\n",
    "\n",
    "A classic implementation of word embeddings is [word2vec](https://en.wikipedia.org/wiki/Word2vec), created in 2013 by a Google team under the direction of Tomas Mikolov ([Mikolov e.a. 2013](https://arxiv.org/abs/1301.3781)). They used a large corpus of texts in English, derived from the Web, and trained their model with a neural network. They found that their technique not only assigned similar vectors to similar weords, but also encoded in those vectors semantic components such as \"male\" or \"female.\" The classic example is \n",
    "\n",
    "            king - male + female ≈ queen\n",
    "            \n",
    "In other words: if you subtract the vector for \"male\" from the vector for \"king\" and add the vector for \"female,\" the nearest neighbor of that computed vector turns out to be the one for \"queen\".\n",
    "\n",
    "Word2vec creeated a revolution in Natural Language Processing and found application in many different tasks. Researchers either use pre-defined word vectors or compute such vectors from their own data. Cuneiformists, of course, do not really have a choice. They have to create their own vectors and here three important drawbacks of word2vec come to light. First, the algorithm works best on very large datasets - the initial implementation used a corpus of 1.6 billion words. For Akkadian or Sumerian such numbers are not feasible, the more so since we may want to build different models for different periods and/or different text types. Second, the neural network architecture behind word2vec works, but it is hard to explain why it works or how. In other words, there is something like a black box between the raw data and the resulting vectors which may be OK for industry applications, but is hard to deal with in a scholarly context. Third, training a model in word2vec (or any similar algorithm) takes a considerable amount of computing time. Since there are many parameters (such as the window size that defines the context of a word) and such settings may dramatically change the results, it is necessary to build and compare many models. Building and comparing many models is doable and common in a Computer Science or NLP research setting, but in Assyriology research that is hardly feasible.\n",
    "\n",
    "More recently researchers have proposed a simpler approach to training word vectors ([Moody 2017](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/), and see [Levy and Goldberg 2014](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html)). This approach uses PMI (Pointwise Mutual Information) combined with SVD (Singular Vector Decomposition), two well-understood and relatively straightforward processes. PMI is used to create a score between every pair of unique words in the corpus based on their individual frequency on the one hand, and the frequency of the two words as collocates. This results in huge matrix of *n* rows and *n* column, where *n* represents the number of unique words in the corpus. SVD is then used to reduce this matrix to a set of *m* dimensional vectors (one vector for each word in the corpus), where *m* may be in the range between, say, 20 and 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1.1 Data Acquisition\n",
    "For demonstration purposes we will use the corpus of Ur III texts, dating to the last century of the third millennium and overwhelmingly written in Sumerian. The great majority of these texts are administrative in nature and record the income and expenses of government offices. Smaller text groups include letters, contracts, and records of litigation (royal inscriptions, literary texts, and incantations are excluded). At the moment of writing more than 100,000 such texts from the Ur III period are known. More than 72,000 of these are edited in the [ORACC](http://oracc.org) project [epsd2/admin/ur3](http://oracc.org/epsd2/admin/ur3). Most of these texts were originally transcribed for [CDLI](http://cdli.ucla.edu) by various contributors; the [CDLI](http://cdli.ucla.edu) editions were imported into [ORACC](http://oracc.org) where they were lemmatized for [ePSD2](http://oracc.org/epsd2) by Steve Tinney and Niek Veldhuis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading JSON\n",
      "Saving http://build-oracc.museum.upenn.edu/json/epsd2-admin-ur3.zip as jsonzip/epsd2-admin-ur3.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd72d6b6d15a4ddabae02ee61fe362b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=577877494.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing JSON\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9377868e5b4ad9ab9379d7e801b8ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=71712.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsd2/admin/ur3/P143238 is not available or not complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "directories = ['jsonzip', 'output', 'corpus']\n",
    "make_dirs(directories)\n",
    "project = \"epsd2/admin/ur3\" # the function oracc_download() expects a list\n",
    "df = get_data(project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Data Formatting\n",
    "\n",
    "The `get_data()` function in `utils` returns a DataFrame in which each word (lemma) is a row. We can manipulate the dataframe with the same methods we have used in previous chapters.\n",
    "\n",
    "> This is not the most efficient way of acquiring and formatting the data. One could adjust the `parsejson()` function discussed in Chapter 2.1 to select the data and directly format that data in a way that can be used in the next section. However, this formatting will likely be slightly different for different corpora. For the Ur III data one may want to exclude numbers and year names, but that is not a concern when dealing with literary texts. For that reason, the code in this section builds the full DataFrame and manipulates the data in that format.\n",
    "\n",
    "The first line in the code below creates the `lemma` column in the format **lugal\\[king\\]N**, as we have done in other chapters. The other lines change or remove specific types of rows:\n",
    "* If there is no Citation Form, replace `lemma` by underscore. This deals with unlemmatized words - words that are damaged or unknown. Such words do not contribute to our analysis, but should also not be removed because we do not want to create artificial neighbors.\n",
    "* If Part of Speech is \"n\" (Number), remove. Numbers are of no interest here and are removed.\n",
    "* If Citation Form contains an \"x\" or \"X\" replace `lemma` by underscore. This is common in damaged personal names. The word is marked as a PN in the lemmatization, but since it is only partly preserved the Citation Form looks like Lu₂.x.\n",
    "* If \"ftype\" = \"yn\": remove all words that belong to year names. Although year names are important for a variety of reasons, they do not contribute \n",
    "* Use the `state` column to indicate logical breaks (indicated by horizontal rulings and the like) and physical breaks in the text.\n",
    "* Include only words in Sumerian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_break = ['illegible', 'traces', 'missing', 'effaced']\n",
    "logical_break = ['other', 'blank', 'ruling']\n",
    "df[\"lemma\"] = df['cf'] + '[' + df['gw'] + ']' + df['pos']\n",
    "df.loc[df[\"cf\"] == \"\" , 'lemma'] = \"_\" \n",
    "df.loc[df[\"cf\"].str.contains(\"x|X\"), 'lemma'] = '_'\n",
    "df.loc[df[\"state\"].isin(logical_break), 'lemma'] = \"break_logical\"\n",
    "df.loc[df[\"state\"].isin(physical_break), 'lemma'] = \"break_physical\"\n",
    "df.loc[df[\"lemma\"].str.startswith(\"break_\"), \"lang\"] = \"sux\"\n",
    "df = df.loc[df[\"pos\"] != \"n\"]\n",
    "df = df.loc[df[\"ftype\"] != \"yn\"]\n",
    "df = df.loc[df[\"lang\"].str.startswith(\"sux\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now create one row per text with the pandas `grouby()` and `aggregate()`functions. The entire text of the `lemma`column is put in the variable `text`as a single string, with line breaks between the individual texts. We add additional line breaks to replace \"break_logical\" and \"break_physical\". This will make sure that the moving window of the PMI process does not cross a break. Then we use`strip()` to remove white space and/or line breaks at the beginning or the end of the entire string. Finally, the string is made into a list, by splitting on line breaks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_bytext = df.groupby(\"id_text\").agg({\"lemma\" : \" \".join})\n",
    "text = \"\\n\".join(df_bytext[\"lemma\"]) \n",
    "b = re.compile(\"break_[^ ]+\")\n",
    "t = re.sub(b, \"\\n\", text).strip()\n",
    "l = t.split(\"\\n\")\n",
    "lemm_ = []\n",
    "for text in l: \n",
    "    text = text.strip()\n",
    "    t = text.split()\n",
    "    lemm_.append(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Klibisz is working with *titles* he compares all possible bigrams and does not define a moving window. That probably needs to change. Aleksi creates windows with the following code:\n",
    "```python\n",
    "wz = self.windowsize - 1\n",
    "zip(*[text[i:] for i in range(1+wz*2)])\n",
    "```\n",
    "This will zip multiple versions of the same text - the first starts at word 0, the next at word 1, etc. Since the window is symmetric (counting forward and backward) take it twice .\n",
    "An alternative method is to use NLTK ngrams, which will create windows.\n",
    "```python\n",
    "from nltk import ngrams\n",
    "n = 1+wz*2\n",
    "windows = ngrams(text, n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.040 seconds (0.00015 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2a. Compute unigram and bigram counts.\n",
    "# A unigram is a single word (x). A bigram is a pair of words (x,y).\n",
    "# Bigrams are counted for any two terms occurring in the same title.\n",
    "# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n",
    "# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\n",
    "t0 = time()\n",
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "for text in lemm_:\n",
    "    cx.update(text)\n",
    "\n",
    "    # Count all pairs of words, even duplicate pairs.\n",
    "    windows = ngrams(text, 7) # 7 is window length - needs to be changeable\n",
    "    for w in windows: # this creates the windows\n",
    "        z = [tuple(l) for l in map(sorted, combinations(w, 2))]\n",
    "        cxy.update(z)\n",
    "\n",
    "#     # Alternative: count only 2-grams.\n",
    "#     for x, y in zip(text[:-1], text[1:]):\n",
    "#         cxy[(x, y)] += 1\n",
    "\n",
    "#     # Alternative: count all pairs of words, but don't double count.\n",
    "#     for x, y in set(map(tuple, map(sorted, combinations(text, 2)))):\n",
    "#         cxy[(x,y)] += 1\n",
    "\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / len(lemm_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21149 tokens before\n",
      "0.012 seconds (0.00000 / iter)\n",
      "10931 tokens after\n",
      "Most common: [('_', 165829), ('sila[unit]N', 94854), ('giŋ[unit]N', 58564), ('itud[moon]N', 52699), ('dumu[child]N', 45917), ('gur[unit]N', 44814), ('ki[place]N', 44124), ('ud[sun]N', 42837), ('udu[sheep]N', 34937), ('šuniŋin[circular]N', 33716), ('kaš[beer]N', 33515), ('še[barley]N', 32950), ('2[00]PN', 30064), ('kišib[seal]N', 27758), ('ninda[bread]N', 27062), ('lugal[king]N', 24310), ('i[oil]N', 22746), ('gud[ox]N', 22083), ('ŋuruš[male]N', 20810), ('dubsar[scribe]N', 20173), ('šu[hand]N', 17854), ('niga[fattened]V/i', 17526), ('ugula[overseer]N', 16437), ('ŋiri[foot]N', 16015), ('sila[lamb]N', 15922)]\n"
     ]
    }
   ],
   "source": [
    "# 2b. Remove frequent and infrequent unigrams.\n",
    "# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n",
    "# very frequently or infrequently. This decreases the vocab size substantially.\n",
    "print('%d tokens before' % len(cx))\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "min_count = 2\n",
    "max_count = sx\n",
    "for x in list(cx.keys()):\n",
    "    if cx[x] < min_count or cx[x] > max_count:\n",
    "        del cx[x]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))\n",
    "print('%d tokens after' % len(cx))\n",
    "print('Most common:', cx.most_common()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.257 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2c. Remove frequent and infrequent bigrams.\n",
    "# Any bigram containing a unigram that was removed must now be removed.\n",
    "t0 = time()\n",
    "for x, y in list(cxy.keys()):\n",
    "    if x not in cx or y not in cx:\n",
    "        del cxy[(x, y)]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.005 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 3. Build unigram <-> index lookup.\n",
    "t0 = time()\n",
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.009 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 4. Sum unigram and bigram counts for computing probabilities.\n",
    "# i.e. p(x) = count(x) / sum(all counts).\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "sxy = sum(cxy.values())\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / (len(cx) + len(cxy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.312 seconds (0.00000 / iter)\n",
      "628620 non-zero elements\n",
      "Sample PPMI values\n",
      " [(('ba.ba₆.ba.an.zi.ge[00]PN', 'lamma.e.silim.mu[00]PN'), 12.987668656041233),\n",
      " (('Gir₂.ba.du[00]PN', 'I.di₃.ni.šu[00]PN'), 12.987668656041233),\n",
      " (('Ur.me.me.ka.bi[00]PN', 'ahulŋal[mistreatment]N'), 12.900657279051602),\n",
      " (('Ba.ša.an.ti.ba.at[00]PN', 'Mu.lu.uš[00]PN'), 12.805347099247278),\n",
      " (('Ha.ur₂.na.ni.gi₄[00]PN', 'La.ap.hi[00]PN'), 12.805347099247278),\n",
      " (('A.za.ba.an[00]PN', 'Mi.iš.hi.ni.iš.hi[00]PN'), 12.805347099247278),\n",
      " (('A.za.ba.an[00]PN', 'Ku.uš.lil₂.li[00]PN'), 12.805347099247278),\n",
      " (('Du.ga.ma.aš.ti[00]PN', 'Mi.šu.a.bi.ir[00]PN'), 12.805347099247278),\n",
      " (('Ga.ba.ba[00]PN', 'Mu.ki.iš[00]SN'), 12.805347099247278),\n",
      " (('Geme₂.lu₅[00]PN', 'lamma.ri.sa.mu[00]PN'), 12.805347099247278)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n",
    "# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n",
    "# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n",
    "# PPMI = max(log(p(x,y) / (p(x) * p(y))), 0)\n",
    "# The probabilities are computed on the fly using the sums from above.\n",
    "t0 = time()\n",
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in cxy.items():\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    data.append(max(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)), 0))\n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PPMI = csc_matrix((data, (rows, cols)))\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\n",
    "print('%d non-zero elements' % PPMI.count_nonzero())\n",
    "print('Sample PPMI values\\n', pformat(pmi_samples.most_common()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.213 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. Factorize the PPMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n",
    "# This part replaces the stochastic gradient descent used by Word2vec\n",
    "# and other related neural network formulations. We pick an arbitrary vector size k=20.\n",
    "t0 = time()\n",
    "U, _, _ = svds(PPMI, k=20)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n",
    "# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\n",
    "t0 = time()\n",
    "norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\n",
    "U /= np.maximum(norms, 1e-7)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "suhur[carp]N, 77\n",
      " (tun[container]N, 0.929) (niŋki[fish]N, 0.942) (saŋkur[fish]N, 0.952) (niŋbuna[turtle]N, 0.924) (saŋkešed[fish]N, 0.923) \n",
      "----------\n",
      "gigir[chariot]N, 295\n",
      " (dus[bathroom]N, 0.834) (guza[chair]N, 0.850) (emarru[quiver]N, 0.779) (gal[cup]N, 0.754) (banšur[table]N, 0.756) \n",
      "----------\n",
      "šah[pig]N, 480\n",
      " (Šu.bar[00]PN, 0.977) (u[goose]N, 0.939) (tumgur[bird]N, 0.930) (zeda[piglet]N, 0.926) (pešgi[rodent]N, 0.926) \n",
      "----------\n",
      "Inanak[1]DN, 264\n",
      " (Du.du[00]PN, 0.775) (Ka₅.a[00]PN, 0.770) (E₂.gissu[00]PN, 0.764) (Lugal.me.lam₂[00]PN, 0.742) (AŠ[00]PN, 0.720) \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 8. Show some nearest neighbor samples as a sanity-check.\n",
    "# The format is <unigram> <count>: (<neighbor unigram>, <similarity>), ...\n",
    "# From this we can see that the relationships make sense.\n",
    "k = 5\n",
    "for x in ['suhur[carp]N', 'gigir[chariot]N', 'šah[pig]N', 'Inanak[1]DN']:\n",
    "    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n",
    "    s = ''\n",
    "    # Compile the list of nearest neighbor descriptions.\n",
    "    # Argpartition is faster than argsort and meets our needs.\n",
    "    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:\n",
    "        if i2x[i] == x: continue\n",
    "        xy = tuple(sorted((x, i2x[i])))\n",
    "        s += '(%s, %.3lf) ' % (i2x[i], dd[i])\n",
    "    print('%s, %d\\n %s' % (x, cx[x], s))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
