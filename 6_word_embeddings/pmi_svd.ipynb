{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Ur III Commodities and Actors: Word Embeddings\n",
    "\n",
    "The basic idea behind word embeddings is that word meaning is determined by the contexts in which the word is found. Or, in the famous quote by the linguist [J.R. Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth): \"You shall know a word by the company it keeps.\" Each unique word (or lemma) in a corpus is assigned a vector in such a way that words attested in similar contexts receive similar vectors. As a result, vectors that represent words with similar meanings become neighbors: they are relatively close in the vector space.\n",
    "\n",
    "A classic implementation of word embeddings is [word2vec](https://en.wikipedia.org/wiki/Word2vec), created in 2013 by a Google team under the direction of Tomas Mikolov ([Mikolov e.a. 2013](https://arxiv.org/abs/1301.3781)). They used a large corpus of texts in English, derived from the Web, and trained their model with a neural network. They found that their technique not only assigned similar vectors to similar words, but also encoded in those vectors semantic components such as \"male\" or \"female.\" The classic example is \n",
    "\n",
    "            king - male + female ≈ queen\n",
    "            \n",
    "In other words: if you subtract the vector for \"male\" from the vector for \"king\" and add the vector for \"female,\" the nearest neighbor of that computed vector turns out to be the one for \"queen\".\n",
    "\n",
    "Word2vec created a revolution in Natural Language Processing and found application in many different tasks. Researchers either use pre-defined word vectors or compute such vectors from their own data. Cuneiformists, of course, do not really have a choice. They have to create their own vectors and here three important drawbacks of word2vec come to light. First, the algorithm works best on very large datasets - the initial implementation used a corpus of 1.6 billion words. For Akkadian or Sumerian such numbers are not feasible, the more so since we may want to build different models for different periods and/or different text types. Second, the neural network architecture behind word2vec works, but it is hard to explain why it works or how. In other words, there is something like a black box between the raw data and the resulting vectors which may be OK for industry applications, but is hard to deal with in a scholarly context. Third, training a model in word2vec (or any similar algorithm) takes a considerable amount of computing time. Since there are many parameters (such as the window size that defines the context of a word) and such settings may dramatically change the results, it is necessary to build and compare many models. Building and comparing many models is doable and common in a Computer Science or NLP research setting, but in Assyriology research that is rarely feasible.\n",
    "\n",
    "More recently researchers have proposed a simpler approach to training word vectors ([Moody 2017](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/), and see [Levy and Goldberg 2014](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html)). This approach uses PMI (Pointwise Mutual Information) combined with SVD (Singular Vector Decomposition), two well-understood and relatively straightforward processes. PMI is used to create a score between every pair of unique words in the corpus based on their individual frequency on the one hand, and the frequency of the two words as collocates on the other hand. A high score means that the words occur more often together than what be expected given the frequency of each word individually. This results in huge matrix of *n* rows and *n* column, where *n* represents the number of unique words in the corpus. SVD is then used to reduce this matrix to a set of *m* dimensional vectors (one vector for each word in the corpus), where *m* may be in the range between, say, 20 and 200. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data Acquisition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data format that we need for computing the PMI matrix is a list of lists, where each list represents a consecutive sequence of lemmas. \"Consecutive\" is crucial here: we do not want the last word of text a to be counted as a collocate of the first word of text b. Similarly, we should take textual breaks into account: after each break, we should start a new sequence of consecutive words. Once we have created this data format we can define a sliding window of *n* words (where *n* is larger than 1 and usually between 2 and 15), so that each word inside a window is considered a collacate of all the others. That will provide us with the list of pairs of words for which PMI scores should be computed.\n",
    "\n",
    "The data acquisition process discussed in Chapter 2.1 transforms the ORACC JSON files into a DataFrame, where each row represents a single word and each column the various data elements that describe a lemma (such as the language, the text ID, the Citation Form, etc.). That DataFrame contains all the information we need and we may transform that DataFrame into the data format we need (the list of lists) with the `pandas` methods discussed in previous chapters. However, this is a rather inefficient approach. Most of the information in the DataFrame we do not need and manipulating a DataFrame with `groupby()` and `aggregate()` is computationally expensive. Instead, we may parse the JSON files in such a way that we create the data format we need directly, without using a `pandas` DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "Each individual document may be regarded as a consecutive sequence of lemmas. Within a document, however, there are two types of breaks: physical breaks and logical breaks. A physical break is an actual break in a clay tablet. A logical break is a horizontal ruling, or the transition from the text of the document to the text of the seal impression. We want to prevent the sliding window to extent over such breaks.\n",
    "\n",
    "In addition, there are several types of words that require a special treatment.\n",
    "* Unlemmatized words - words that are damaged or unknown should not be included. They are replaced by an underscore (\"_\"). Such words do not contribute to our analysis, but should also not be removed because we do not want to create artificial neighbors.\n",
    "* Numbers are of no interest here and are entirely removed.\n",
    "* Damaged personal names are lemmatized as PN with a citation form in the format Lu₂.x. Those are treated the same way as unlemmatized words (replace by underscore)\n",
    "* Year names are removed. Although year names are important for a variety of reasons, they belong to a different register of Sumerian and their collocates are of no interest here. \n",
    "* Remove words that are not in Sumerian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output', 'corpus']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"epsd2/admin/ur3\"\n",
    "p = [projects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = format_project_list(projects)\n",
    "oracc_download(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parsejson()` function below works in a way that is similar to the `parsejson()` functions we discussed in Chapter 2. Each .json file to be parsed represents a single text. The list `l` collects lemmatizations in the format CF\\[GW\\]POS (for instance lugal\\[king\\]N). When the function has gone through the entire file it appends the list `l` to the list `lemm_l` that is defined in the next cell (where `parsejson()` is called). This will create a list of list (`lemm_l`, where each lower order list represents a single text.\n",
    "\n",
    "Breaks in the text (both logical breaks and physical breaks) are marked in the JSON with a `state` node. This node has a restricted vocabulary to indicate breaks, traces, illegible lines, horizontal rulings, etc. The vocabulary that marks a logical or physical break is assembled in the list `breakage` (define in the next cell). When such a node is encountered, the list `l` is added to the list of lists `lemma_l` and a new (empty) `l` list is created. Then the process resumes. This way, a tablet with breaks is chopped up into multiple lists of consecutive lemmas.\n",
    "\n",
    "The rest of the function takes care of special situations:\n",
    "* Unlemmatized words - words that are damaged or unknown should not be included. They are replaced by an underscore (\"_\"). Such words do not contribute to our analysis, but should also not be removed because we do not want to create artificial neighbors.\n",
    "* Numbers are of no interest here and are entirely removed.\n",
    "* Damaged personal names are lemmatized as PN with a citation form in the format Lu₂.x. Those are treated the same way as unlemmatized words (replace by underscore)\n",
    "* Year names are removed. Year names are important for dating, for political history and for understanding the ideology of the period. They do not contribute meaningful collocates to the transactions in the documents studied here.\n",
    "* Remove words that are not in Sumerian\n",
    "\n",
    "The process also skips lemmas that derive from the \"sign\" and \"pronunciation\" columns of lexical lists. That is not relevant in the current chapter, but may become relevant if you wish to use this code on a wider set of texts.\n",
    "\n",
    "The `parsejson()` function returns the list `l`, a list of consecutive lemmas; this list is then appended to the list `lemm_l`. If the function encounters a break in the text, the lemmas that have been collected in the list `l` are appended to `lemm_l` and the list `l` is cleared. Note that, in order to do so, we need to append a *copy* of `l` (rathjer than a view) or otherwise clearing `l` will also clear the view of `l` that was appended to `lemm_l`. This is done by appending `l.copy()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "l = []\n",
    "ids_ = []\n",
    "breakage = ['illegible', 'traces', 'missing', 'effaced','other', 'blank', 'ruling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        elif JSONobject.get(\"state\", \"\") in breakage:\n",
    "            if len(l) > 1:\n",
    "                lemm_l.append(l.copy()) #append a copy of l\n",
    "            l.clear()\n",
    "            continue\n",
    "        elif JSONobject.get(\"subtype\",\"\") in ['sg', 'pr']: # skip the fields \"sign\" and \"pronunciation\"\n",
    "            continue                                     # in lexical texts\n",
    "        elif JSONobject.get(\"subtype\", \"\")[:5] in [\"seal \", \"envel\"]: # seal 1, seal 2, etc. or envelope\n",
    "            if len(l) > 1:\n",
    "                lemm_l.append(l.copy())\n",
    "            l.clear()\n",
    "            continue\n",
    "        elif JSONobject.get(\"ftype\", \"\") == \"yn\":\n",
    "            continue # skip year names\n",
    "        elif \"f\" in JSONobject: \n",
    "            word = JSONobject[\"f\"]\n",
    "            if word[\"lang\"][:3] != \"sux\": #only Sumerian and Emesal\n",
    "                continue\n",
    "            if word.get(\"pos\", \"\") == \"n\":  # omit numbers\n",
    "                continue\n",
    "            if \"cf\" in word:\n",
    "                #for some reason some words appear without pos. Provisionally treated as Noun\n",
    "                lemm = f\"{word['cf']}[{word['gw']}]{word.get('pos', 'N')}\"\n",
    "                lemm = lemm.replace(' ', '-') # remove commas and spaces from lemm\n",
    "                lemm = lemm.replace(',', '')\n",
    "            else:\n",
    "                lemm = \"_\" # if word is unlemmatized enter a place holder\n",
    "            if \"x\" in word.get(\"cf\",\"\").lower():  # partly damaged PN; enter placeholder\n",
    "                lemm = \"_\"\n",
    "            l.append(lemm)\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54dc2eb05b2f435089e3aaee3ad7ecd1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=71712.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsd2/admin/ur3/P143238 is not available or not complete\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc = project):                            #iterate over the file names\n",
    "        id_no = filename[-13:-5]\n",
    "        if id_no in ids_ and not \"X\" in id_no: # Check if P/Q number is already in there\n",
    "            continue        # a text may appear in multiple projects\n",
    "        id_text = project + id_no # id_text is, for instance, blms/P414332\n",
    "        ids_.append(id_text)\n",
    "        l = []\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            l = parsejson(data_json)\n",
    "            if len(l) > 1:\n",
    "                lemm_l.append(l.copy())\n",
    "            l.clear()\n",
    "        except:\n",
    "            print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in the list of lists lemm_l, which holds the individual texts. Each document is represented by one or more lists of lemmas, with the lemmas in the original order. Secondly, the list ids_ holds all the text IDs. These IDs are not further used, but were collected to prevent duplication, which may be an issue if you derive data from more than one project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is directly taken from the blog post \"simple word vectors with co-occurrence pmi and svd\" by Alex Klibisz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Klibisz is working with *titles* he compares all possible bigrams and does not define a moving window. That probably needs to change. Aleksi creates windows with the following code:\n",
    "```python\n",
    "wz = self.windowsize - 1\n",
    "zip(*[text[i:] for i in range(1+wz*2)])\n",
    "```\n",
    "This will zip multiple versions of the same text - the first starts at word 0, the next at word 1, etc. Since the window is symmetric (counting forward and backward) take it twice .\n",
    "An alternative method is to use NLTK ngrams, which will create windows.\n",
    "```python\n",
    "from nltk import ngrams\n",
    "n = 7\n",
    "windows = ngrams(text, n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.269 seconds (0.00017 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2a. Compute unigram and bigram counts.\n",
    "# A unigram is a single word (x). A bigram is a pair of words (x,y).\n",
    "# Bigrams are counted for any two terms occurring in the same title.\n",
    "# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n",
    "# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\n",
    "t0 = time()\n",
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "for text in lemm_l:\n",
    "    cx.update(text)\n",
    "\n",
    "    # Count all pairs of words, even duplicate pairs.\n",
    "    windows = ngrams(text, 7) # 7 is window length - needs to be changeable\n",
    "    for w in windows: # this creates the windows\n",
    "        z = [tuple(l) for l in map(sorted, combinations(w, 2))]\n",
    "        cxy.update(z)\n",
    "\n",
    "#     # Alternative: count only 2-grams.\n",
    "#     for x, y in zip(text[:-1], text[1:]):\n",
    "#         cxy[(x, y)] += 1\n",
    "\n",
    "#     # Alternative: count all pairs of words, but don't double count.\n",
    "#     for x, y in set(map(tuple, map(sorted, combinations(text, 2)))):\n",
    "#         cxy[(x,y)] += 1\n",
    "\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / len(lemm_l)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21154 tokens before\n",
      "0.008 seconds (0.00000 / iter)\n",
      "21147 tokens after\n",
      "Most common: [('sila[unit]N', 190943), ('itud[moon]N', 167604), ('dumu[child]N', 154024), ('ki[place]N', 150448), ('ud[sun]N', 147854), ('udu[sheep]N', 132822), ('giŋ[unit]N', 117280), ('dubsar[scribe]N', 102538), ('gur[unit]N', 89679), ('dab[seize]V/t', 84179), ('A.kal.la[00]PN', 73218), ('mašgal[goat]N', 71888), ('Ab.ba.sa₆.ga[00]PN', 69810), ('še[barley]N', 67822), ('šuniŋin[circular]N', 67432), ('kaš[beer]N', 67218), ('Ezemmah[1]MN', 65814), ('Ur.ge₆.par₄[00]PN', 63268), ('En.dingir.mu[00]PN', 62956), ('2[00]PN', 60922), ('kišib[seal]N', 55541), ('ninda[bread]N', 54162), ('lugal[king]N', 48617), ('i[oil]N', 45496), ('gud[ox]N', 44559)]\n"
     ]
    }
   ],
   "source": [
    "# 2b. Remove frequent and infrequent unigrams.\n",
    "# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n",
    "# very frequently or infrequently. This decreases the vocab size substantially.\n",
    "print('%d tokens before' % len(cx))\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "min_count = 2\n",
    "max_count = sx\n",
    "for x in list(cx.keys()):\n",
    "    if cx[x] < min_count or cx[x] > max_count:\n",
    "        del cx[x]\n",
    "del cx['_']\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))\n",
    "print('%d tokens after' % len(cx))\n",
    "print('Most common:', cx.most_common()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.260 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2c. Remove frequent and infrequent bigrams.\n",
    "# Any bigram containing a unigram that was removed must now be removed.\n",
    "t0 = time()\n",
    "for x, y in list(cxy.keys()):\n",
    "    if x not in cx or y not in cx:\n",
    "        del cxy[(x, y)]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 3. Build unigram <-> index lookup.\n",
    "t0 = time()\n",
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 4. Sum unigram and bigram counts for computing probabilities.\n",
    "# i.e. p(x) = count(x) / sum(all counts).\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "sxy = sum(cxy.values())\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / (len(cx) + len(cxy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.590 seconds (0.00000 / iter)\n",
      "772137 non-zero elements\n",
      "Sample PPMI values\n",
      " [(('Me.ra.iškur[00]PN', 'U₃.da.bi[00]FN'), 13.824340172249912),\n",
      " (('Kunga.ar₃[00]SN', 'Li.pi₂.it.ištar[00]PN'), 13.824340172249912),\n",
      " (('Geme₂.sal₄.tug₂.u₁₈[00]PN', 'ba.ba₆.ku₃[00]PN'), 13.824340172249912),\n",
      " (('Nin.e₂.nun[00]PN', 'Nin.ir.ra.an.gi₄[00]PN'), 13.824340172249912),\n",
      " (('Na.mu.ši.bar[00]PN', 'Nin.e.ba.zi[00]PN'), 13.824340172249912),\n",
      " (('E.pu.uq[00]PN', 'Eš₁₈.dar.ka₃.li₂.su[00]PN'), 13.824340172249912),\n",
      " (('Geme₂.ša₃.ku₃.ga[00]PN', 'Nin.kal.la.ar[00]PN'), 13.824340172249912),\n",
      " (('Di.ta.nu.sar[00]PN', 'Nam.lu₂[00]PN'), 13.824340172249912),\n",
      " (('Lu₂.zi.in.i₃.zu[00]PN', 'Nimgir.nam.gi.na[00]PN'), 13.824340172249912),\n",
      " (('A.ab.gu.la[00]PN', 'Ur.pa₅.sir₂.ra[00]PN'), 13.824340172249912)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n",
    "# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n",
    "# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n",
    "# PPMI(x,y) = max(log(p(x,y) / (p(x) * p(y))), 0)\n",
    "# The probabilities are computed on the fly using the sums from above.\n",
    "t0 = time()\n",
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in cxy.items():\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    data.append(max(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)), 0))\n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PPMI = csc_matrix((data, (rows, cols)))\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\n",
    "print('%d non-zero elements' % PPMI.count_nonzero())\n",
    "print('Sample PPMI values\\n', pformat(pmi_samples.most_common()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.316 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. Factorize the PPMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n",
    "# This part replaces the stochastic gradient descent used by Word2vec\n",
    "# and other related neural network formulations. We pick an arbitrary vector size k=20.\n",
    "t0 = time()\n",
    "U, _, _ = svds(PPMI, k=20)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.002 seconds\n"
     ]
    }
   ],
   "source": [
    "# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n",
    "# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\n",
    "t0 = time()\n",
    "norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\n",
    "U /= np.maximum(norms, 1e-7)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esir[shoe]N, 416\n",
      " (kila[weight]N, 0.885) (tugdua[felt]N, 0.831) (dušia[stone]N, 0.821) (eban[pair]N, 0.928) (KA.GUM×ŠE[~leather]N, 0.874) \n",
      "----------\n",
      "kugsig[gold]N, 2188\n",
      " (zahum[basin]N, 0.987) (zagaltum[object]N, 0.986) (nir[stone]N, 0.986) (zagin[lapis]N, 0.986) (sua[stone]N, 0.985) \n",
      "----------\n",
      "lugal[king]N, 48617\n",
      " (lu[person]N, 0.903) (muhaldim[cook]N, 0.900) (Še.eh.la.am[00]PN, 0.873) (ziga[expenditure]N, 0.850) (ragaba[rider]N, 0.849) \n",
      "----------\n",
      "Inana[1]DN, 3202\n",
      " (Ninlil[1]DN, 0.941) (Nanaya[1]DN, 0.939) (An.nu.ni.tum[00]PN, 0.895) (Ištaran[1]DN, 0.894) (Ningal[1]DN, 0.887) \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 8. Show some nearest neighbor samples as a sanity-check.\n",
    "# The format is <unigram> <count>: (<neighbor unigram>, <similarity>), ...\n",
    "# From this we can see that the relationships make sense.\n",
    "k = 5\n",
    "for x in ['esir[shoe]N', 'kugsig[gold]N', 'lugal[king]N', 'Inana[1]DN']:\n",
    "    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n",
    "    s = ''\n",
    "    # Compile the list of nearest neighbor descriptions.\n",
    "    # Argpartition is faster than argsort and meets our needs.\n",
    "    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:\n",
    "        if i2x[i] == x: continue\n",
    "        xy = tuple(sorted((x, i2x[i])))\n",
    "        s += '(%s, %.3lf) ' % (i2x[i], dd[i])\n",
    "    print('%s, %d\\n %s' % (x, cx[x], s))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Ur III Commodities and Actors: Word Embeddings\n",
    "\n",
    "The basic idea behind word embeddings is that word meaning is determined by the contexts in which the word is found. Or, in the famous quote by the linguist [J.R. Firth](https://en.wikipedia.org/wiki/John_Rupert_Firth): \"You shall know a word by the company it keeps.\" Each unique word (or lemma) in a corpus is assigned a vector in such a way that words attested in similar contexts receive similar vectors. As a result, vectors that represent words with similar meanings become neighbors: they are relatively close in the vector space.\n",
    "\n",
    "A classic implementation of word embeddings is [word2vec](https://en.wikipedia.org/wiki/Word2vec), created in 2013 by a Google team under the direction of Tomas Mikolov ([Mikolov e.a. 2013](https://arxiv.org/abs/1301.3781)). They used a large corpus of texts in English, derived from the Web, and trained their model with a neural network. They found that their technique not only assigned similar vectors to similar weords, but also encoded in those vectors semantic components such as \"male\" or \"female.\" The classic example is \n",
    "\n",
    "            king - male + female ≈ queen\n",
    "            \n",
    "In other words: if you subtract the vector for \"male\" from the vector for \"king\" and add the vector for \"female,\" the nearest neighbor of that computed vector turns out to be the one for \"queen\".\n",
    "\n",
    "Word2vec creeated a revolution in Natural Language Processing and found application in many different tasks. Researchers either use pre-defined word vectors or compute such vectors from their own data. Cuneiformists, of course, do not really have a choice. They have to create their own vectors and here three important drawbacks of word2vec come to light. First, the algorithm works best on very large datasets - the initial implementation used a corpus of 1.6 billion words. For Akkadian or Sumerian such numbers are not feasible, the more so since we may want to build different models for different periods and/or different text types. Second, the neural network architecture behind word2vec works, but it is hard to explain why it works or how. In other words, there is something like a black box between the raw data and the resulting vectors which may be OK for industry applications, but is hard to deal with in a scholarly context. Third, training a model in word2vec (or any similar algorithm) takes a considerable amount of computing time. Since there are many parameters (such as the window size that defines the context of a word) and such settings may dramatically change the results, it is necessary to build and compare many models. Building and comparing many models is doable and common in a Computer Science or NLP research setting, but in Assyriology research that is hardly feasible.\n",
    "\n",
    "More recently researchers have proposed a simpler approach to training word vectors ([Moody 2017](https://multithreaded.stitchfix.com/blog/2017/10/18/stop-using-word2vec/), and see [Levy and Goldberg 2014](https://papers.nips.cc/paper/2014/hash/feab05aa91085b7a8012516bc3533958-Abstract.html)). This approach uses PMI (Pointwise Mutual Information) combined with SVD (Singular Vector Decomposition), two well-understood and relatively straightforward processes. PMI is used to create a score between every pair of unique words in the corpus based on their individual frequency on the one hand, and the frequency of the two words as collocates. This results in huge matrix of *n* rows and *n* column, where *n* represents the number of unique words in the corpus. SVD is then used to reduce this matrix to a set of *m* dimensional vectors (one vector for each word in the corpus), where *m* may be in the range between, say, 20 and 200. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['sila[unit]N',\n",
       "  'giŋ[unit]N',\n",
       "  'še[barley]N',\n",
       "  'gur[unit]N',\n",
       "  'ziznumun[seed]N',\n",
       "  'gur[unit]N',\n",
       "  'kibnumun[wheat]N',\n",
       "  'šenumun[seed]N',\n",
       "  'murgu[fodder]N',\n",
       "  'sila[unit]N',\n",
       "  'še[barley]N',\n",
       "  'gur[unit]N',\n",
       "  'a[arm]N',\n",
       "  'luhuŋa[hireling]N',\n",
       "  'ašag[field]N',\n",
       "  'sila[unit]N',\n",
       "  'še[barley]N',\n",
       "  'gur[unit]N',\n",
       "  'a[arm]N',\n",
       "  'luhuŋa[hireling]N',\n",
       "  'sahar[soil]N',\n",
       "  'ki[place]N',\n",
       "  '_',\n",
       "  'ugu[skull]N',\n",
       "  'Da.a.gi₄[00]PN',\n",
       "  'ŋar[place]V/t',\n",
       "  'kišib[seal]N',\n",
       "  'Ur.šara₂[00]PN',\n",
       "  'bisaŋdubak[archivist]N'],\n",
       " ['Ur.ge₆.par₄[00]PN', 'dubsar[scribe]N', 'dumu[child]N', 'A.kal.la[00]PN'],\n",
       " ['udu[sheep]N',\n",
       "  'mašgal[goat]N',\n",
       "  'ud[sun]N',\n",
       "  'ki[place]N',\n",
       "  'Ab.ba.sa₆.ga[00]PN',\n",
       "  'En.dingir.mu[00]PN',\n",
       "  'dab[seize]V/t',\n",
       "  'itud[moon]N',\n",
       "  'Ezemmah[1]MN'],\n",
       " ['sila[lamb]N',\n",
       "  'Utu[1]DN',\n",
       "  'mu.DU[delivery]N',\n",
       "  'Gu₃.de₂.a[00]PN',\n",
       "  'sila[lamb]N',\n",
       "  'en.lil₂[00]PN',\n",
       "  'sila[lamb]N',\n",
       "  'Ninlil[1]DN',\n",
       "  'mu.DU[delivery]N',\n",
       "  'en[priest]N',\n",
       "  'Inana[1]DN',\n",
       "  'sila[lamb]N',\n",
       "  'Ninegalak[1]DN',\n",
       "  'mu.DU[delivery]N',\n",
       "  'ensik[ruler]N',\n",
       "  'Nibru[00]SN',\n",
       "  'zabardab[official]N',\n",
       "  'maškim[administrator]N',\n",
       "  'ud[sun]N',\n",
       "  '2[00]PN',\n",
       "  'ziga[expenditure]N',\n",
       "  'itud[moon]N',\n",
       "  'Ezemninazuk[1]MN'],\n",
       " ['še[barley]N',\n",
       "  'gur[unit]N',\n",
       "  'lugal[king]N',\n",
       "  'niŋsam[price]N',\n",
       "  'udu[sheep]N',\n",
       "  'idub[granary]N',\n",
       "  'ašag[field]N',\n",
       "  'Hu.ba[00]FN',\n",
       "  'ŋar[place]V/t',\n",
       "  'ki[place]N',\n",
       "  'Ba.zi[00]PN',\n",
       "  'Tul₂[00]PN',\n",
       "  'šu[hand]N',\n",
       "  'teŋ[near]V/i']]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemm_l[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['epsd2/admin/ur3/corpusjson/P113959.json',\n",
       " 'epsd2/admin/ur3/corpusjson/P127505.json',\n",
       " 'epsd2/admin/ur3/corpusjson/P125538.json',\n",
       " 'epsd2/admin/ur3/corpusjson/P379198.json',\n",
       " 'epsd2/admin/ur3/corpusjson/P111964.json']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
