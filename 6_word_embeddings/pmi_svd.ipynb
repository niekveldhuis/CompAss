{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data acquisition process discussed in Chapter 2.1 transforms the ORACC JSON files into a DataFrame, where each row represents a single word and each column the various data elements that describe a lemma. The DataFrame then needs to be manipulated to produce the necessary data format. For the current task that is a rather inefficient approach. We may adapt the JSON parser in such a way that it directly produces the data format that can be ingested by the PMI process.\n",
    "\n",
    "The data format we need is a list of list, where each second-order list represents a sequence of consecutive lemmas. We can then create a sliding window of *n* words that will determine which lemmas in that consecutive sequence are treated as collocates.\n",
    "\n",
    "Each individual document may be regarded as a consecutive sequence of lemmas. Within a document, however, there are two types of breaks: physical breaks and logical breaks. A physical break is an actual break in a clay tablet. A logical break is a horizontal ruling, or the transition from the text of the document to the text of the seal impression. We want to prevent the sliding window to extent over such breaks.\n",
    "\n",
    "In addition, there are several types of words that require a special treatment.\n",
    "* Unlemmatized words - words that are damaged or unknown should not be included. They are replaced by an underscore (\"_\"). Such words do not contribute to our analysis, but should also not be removed because we do not want to create artificial neighbors.\n",
    "* Numbers are of no interest here and are entirely removed.\n",
    "* Damaged personal names are lemmatized as PN with a citation form in the format Lu₂.x. Those are treated the same way as unlemmatized words (replace by underscore)\n",
    "* Year names are removed. Although year names are important for a variety of reasons, they belong to a different register of Sumerian and their collocates are of no interest here. \n",
    "* Remove words that are not in Sumerian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output', 'corpus']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Project(s):  treasury\n"
     ]
    }
   ],
   "source": [
    "projects = input('Project(s): ').lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://build-oracc.museum.upenn.edu/json/treasury.zip as jsonzip/treasury.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f88712c0c17047ceafae0e27b09e0686",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='treasury', max=2598034.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "['treasury']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = format_project_list(projects)\n",
    "oracc_download(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    l = []\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        if JSONobject.get(\"state\", \"\") in breakage:\n",
    "            if len(l) > 1:\n",
    "                lemm_.append(l)\n",
    "            l = []\n",
    "            continue\n",
    "        if JSONobject.get(\"subtype\",\"\") in ['sg', 'pr']: # skip the fields \"sign\" and \"pronunciation\"\n",
    "            continue                                     # in lexical texts\n",
    "        if JSONobject.get(\"ftype\", \"\") == \"yn\":\n",
    "            continue # skip year names\n",
    "        if \"f\" in JSONobject: \n",
    "            word = JSONobject[\"f\"]\n",
    "            if word[\"lang\"][:3] != \"sux\": #only Sumerian and Emesal\n",
    "                continue\n",
    "            if word.get(\"pos\", \"\") == \"n\":  # omit numbers\n",
    "                continue\n",
    "            if \"cf\" in word:\n",
    "                #for some reason some words appear without pos. Provisionally treated as Noun\n",
    "                lemm = f\"{word['cf']}[{word['gw']}]{word.get('pos', 'N')}\"\n",
    "                lemm = lemm.replace(' ', '-') # remove commas and spaces from lemm\n",
    "                lemm = lemm.replace(',', '')\n",
    "            else:\n",
    "                lemm = \"_\" # if word is unlemmatized enter a place holder\n",
    "            if \"x\" in word.get(\"cf\",\"\").lower():  # partly damaged PN; enter placeholder\n",
    "                lemm = \"_\"\n",
    "            l.append(lemm)    \n",
    "    if len(l) > 1:\n",
    "        lemm_.append(l)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94f86c37e88b4a03bd57a2d0655fc83c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='treasury', max=298.0, style=ProgressStyle(description_wid…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lemm_ = []\n",
    "ids_ = []\n",
    "breakage = ['illegible', 'traces', 'missing', 'effaced','other', 'blank', 'ruling']\n",
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc = project):                            #iterate over the file names\n",
    "        id_no = filename[-13:-5]\n",
    "        if id_no in ids_ and not \"X\" in id_no: # Check if P/Q number is already in there\n",
    "            continue        # a text may appear in multiple projects\n",
    "        id_text = project + id_no # id_text is, for instance, blms/P414332\n",
    "        ids_.append(id_text)\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            parsejson(data_json)\n",
    "        except:\n",
    "            print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in the list of lists lemm_, which holds the individual texts. Each individual text is represented by a list of lemmas, with the lemmas in the original order. Currently, breaks etc. are not represented. Secondly, the list ids_ holds all the text IDs; the list ids_ has the same order as the list of lists lemm_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is directly taken from the blog post \"simple word vectors with co-occurrence pmi and svd\" by Alex Klibisz."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ready\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "from collections import Counter\n",
    "from itertools import combinations\n",
    "from math import log\n",
    "from pprint import pformat\n",
    "from scipy.sparse import csc_matrix\n",
    "from scipy.sparse.linalg import svds\n",
    "from string import punctuation\n",
    "from time import time\n",
    "from nltk import ngrams\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "print('Ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Klibisz is working with *titles* he compares all possible bigrams and does not define a moving window. That probably needs to change. Aleksi creates windows with the following code:\n",
    "```python\n",
    "wz = self.windowsize - 1\n",
    "zip(*[text[i:] for i in range(1+wz*2)])\n",
    "```\n",
    "This will zip multiple versions of the same text - the first starts at word 0, the next at word 1, etc. Since the window is symmetric (counting forward and backward) take it twice .\n",
    "An alternative method is to use NLTK ngrams, which will create windows.\n",
    "```python\n",
    "from nltk import ngrams\n",
    "n = 1+wz*2\n",
    "windows = ngrams(text, n)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.093 seconds (0.00026 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2a. Compute unigram and bigram counts.\n",
    "# A unigram is a single word (x). A bigram is a pair of words (x,y).\n",
    "# Bigrams are counted for any two terms occurring in the same title.\n",
    "# For example, the title \"Foo bar baz\" has unigrams [foo, bar, baz]\n",
    "# and bigrams [(bar, foo), (bar, baz), (baz, foo)]\n",
    "t0 = time()\n",
    "cx = Counter()\n",
    "cxy = Counter()\n",
    "for text in lemm_:\n",
    "    cx.update(text)\n",
    "\n",
    "    # Count all pairs of words, even duplicate pairs.\n",
    "    windows = ngrams(text, 7) # 7 is window length - needs to be changeable\n",
    "    for w in windows: # this creates the windows\n",
    "        z = [tuple(l) for l in map(sorted, combinations(w, 2))]\n",
    "        cxy.update(z)\n",
    "\n",
    "#     # Alternative: count only 2-grams.\n",
    "#     for x, y in zip(text[:-1], text[1:]):\n",
    "#         cxy[(x, y)] += 1\n",
    "\n",
    "#     # Alternative: count all pairs of words, but don't double count.\n",
    "#     for x, y in set(map(tuple, map(sorted, combinations(text, 2)))):\n",
    "#         cxy[(x,y)] += 1\n",
    "\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / len(lemm_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "935 tokens before\n",
      "0.001 seconds (0.00000 / iter)\n",
      "505 tokens after\n",
      "Most common: [('_', 609), ('itud[moon]N', 283), ('kugsig[gold]N', 263), ('giŋ[unit]N', 257), ('šag[heart]N', 240), ('kugbabbar[silver]N', 239), ('eban[pair]N', 218), ('ki[place]N', 217), ('šu[hand]N', 166), ('teŋ[near]V/i', 137), ('suhub[boots]N', 132), ('zig[rise]V/i', 132), ('mana[unit]N', 129), ('kila[weight]N', 125), ('dušia[stone]N', 114), ('zabar[bronze]N', 107), ('šuniŋin[circular]N', 103), ('Puzrišdagan[1]SN', 102), ('maškim[administrator]N', 100), ('lugal[king]N', 99), ('še[barley]N', 98), ('ŋar[place]V/t', 95), ('har[ring]N', 79), ('Nibru[00]SN', 75), ('esir[shoe]N', 74)]\n"
     ]
    }
   ],
   "source": [
    "# 2b. Remove frequent and infrequent unigrams.\n",
    "# Pick arbitrary occurrence count thresholds to eliminate unigrams occurring\n",
    "# very frequently or infrequently. This decreases the vocab size substantially.\n",
    "print('%d tokens before' % len(cx))\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "min_count = 2\n",
    "max_count = sx\n",
    "for x in list(cx.keys()):\n",
    "    if cx[x] < min_count or cx[x] > max_count:\n",
    "        del cx[x]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))\n",
    "print('%d tokens after' % len(cx))\n",
    "print('Most common:', cx.most_common()[:25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.010 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 2c. Remove frequent and infrequent bigrams.\n",
    "# Any bigram containing a unigram that was removed must now be removed.\n",
    "t0 = time()\n",
    "for x, y in list(cxy.keys()):\n",
    "    if x not in cx or y not in cx:\n",
    "        del cxy[(x, y)]\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 3. Build unigram <-> index lookup.\n",
    "t0 = time()\n",
    "x2i, i2x = {}, {}\n",
    "for i, x in enumerate(cx.keys()):\n",
    "    x2i[x] = i\n",
    "    i2x[i] = x\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cx)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.000 seconds (0.00000 / iter)\n"
     ]
    }
   ],
   "source": [
    "# 4. Sum unigram and bigram counts for computing probabilities.\n",
    "# i.e. p(x) = count(x) / sum(all counts).\n",
    "t0 = time()\n",
    "sx = sum(cx.values())\n",
    "sxy = sum(cxy.values())\n",
    "print('%.3lf seconds (%.5lf / iter)' %\n",
    "      (time() - t0, (time() - t0) / (len(cx) + len(cxy))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.026 seconds (0.00000 / iter)\n",
      "9256 non-zero elements\n",
      "Sample PPMI values\n",
      " [(('barag[sack]N', 'uzud[goat]N'), 7.162393595403842),\n",
      " (('Aradnanna[1]PN', 'Na.wa.ar[00]SN'), 7.0082429155765835),\n",
      " (('anzah[glass]N', 'saŋkul[bolt]N'), 7.0082429155765835),\n",
      " (('habum[garment]N', 'namarum[garment]N'), 7.0082429155765835),\n",
      " (('niŋsua[object]N', 'saŋki[forehead]N'), 7.008242915576583),\n",
      " (('Wa.qar.šu.suen[00]PN', 'baza[dwarf]N'), 7.008242915576583),\n",
      " (('im[clay]N', 'šeŋ[rain]V/i'), 7.008242915576583),\n",
      " (('Eriš₂[00]SN', 'Ur.nin.mug[00]PN'), 7.008242915576583),\n",
      " (('Na.wa.ar[00]SN', 'ere[go]V/i'), 6.825921358782629),\n",
      " (('aŋarak[fluid]N', 'gu[eat]V/t'), 6.825921358782629)]\n"
     ]
    }
   ],
   "source": [
    "# 5. Accumulate data, rows, and cols to build sparse PMI matrix\n",
    "# Recall from the blog post that the PMI value for a bigram with tokens (x, y) is: \n",
    "# PMI(x,y) = log(p(x,y) / p(x) / p(y)) = log(p(x,y) / (p(x) * p(y)))\n",
    "# PPMI = max(log(p(x,y) / (p(x) * p(y))), 0)\n",
    "# The probabilities are computed on the fly using the sums from above.\n",
    "t0 = time()\n",
    "pmi_samples = Counter()\n",
    "data, rows, cols = [], [], []\n",
    "for (x, y), n in cxy.items():\n",
    "    rows.append(x2i[x])\n",
    "    cols.append(x2i[y])\n",
    "    data.append(max(log((n / sxy) / (cx[x] / sx) / (cx[y] / sx)), 0))\n",
    "    pmi_samples[(x, y)] = data[-1]\n",
    "PPMI = csc_matrix((data, (rows, cols)))\n",
    "print('%.3lf seconds (%.5lf / iter)' % (time() - t0, (time() - t0) / len(cxy)))\n",
    "print('%d non-zero elements' % PPMI.count_nonzero())\n",
    "print('Sample PPMI values\\n', pformat(pmi_samples.most_common()[:10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.030 seconds\n"
     ]
    }
   ],
   "source": [
    "# 6. Factorize the PPMI matrix using sparse SVD aka \"learn the unigram/word vectors\".\n",
    "# This part replaces the stochastic gradient descent used by Word2vec\n",
    "# and other related neural network formulations. We pick an arbitrary vector size k=20.\n",
    "t0 = time()\n",
    "U, _, _ = svds(PPMI, k=20)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001 seconds\n"
     ]
    }
   ],
   "source": [
    "# 7. Normalize the vectors to enable computing cosine similarity in next cell.\n",
    "# If confused see: https://en.wikipedia.org/wiki/Cosine_similarity#Definition\n",
    "t0 = time()\n",
    "norms = np.sqrt(np.sum(np.square(U), axis=1, keepdims=True))\n",
    "U /= np.maximum(norms, 1e-7)\n",
    "print('%.3lf seconds' % (time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "esir[shoe]N, 74\n",
      " (eban[pair]N, 0.907) (gudimba[~shoe]N, 0.900) (dušia[stone]N, 0.854) (sadab[laces]N, 0.828) (im[clay]N, 0.810) \n",
      "----------\n",
      "kugsig[gold]N, 263\n",
      " (harhara[~jewelry]N, 0.832) (za.lu₂.NAM.ra[jewelry]N, 0.809) (gu[neck]N, 0.843) (sig[weak]V/i, 0.793) (penzer[genitals]N, 0.816) \n",
      "----------\n",
      "lugal[king]N, 99\n",
      " (gagsum[arrow]N, 0.734) (gag[nail]N, 0.612) (hanni[~bow]AJ, 0.672) (gagsisa[arrow]N, 0.621) (kalag[strong]V/i, 0.605) \n",
      "----------\n",
      "Inana[1]DN, 15\n",
      " (du[build]V/t, 0.701) (Pu.us₂[00]SN, 0.775) (de[pour]V/t, 0.633) (ugunu[decoration]N, 0.566) (igi[eye]N, 0.607) \n",
      "----------\n"
     ]
    }
   ],
   "source": [
    "# 8. Show some nearest neighbor samples as a sanity-check.\n",
    "# The format is <unigram> <count>: (<neighbor unigram>, <similarity>), ...\n",
    "# From this we can see that the relationships make sense.\n",
    "k = 5\n",
    "for x in ['esir[shoe]N', 'kugsig[gold]N', 'lugal[king]N', 'Inana[1]DN']:\n",
    "    dd = np.dot(U, U[x2i[x]]) # Cosine similarity for this unigram against all others.\n",
    "    s = ''\n",
    "    # Compile the list of nearest neighbor descriptions.\n",
    "    # Argpartition is faster than argsort and meets our needs.\n",
    "    for i in np.argpartition(-1 * dd, k + 1)[:k + 1]:\n",
    "        if i2x[i] == x: continue\n",
    "        xy = tuple(sorted((x, i2x[i])))\n",
    "        s += '(%s, %.3lf) ' % (i2x[i], dd[i])\n",
    "    print('%s, %d\\n %s' % (x, cx[x], s))\n",
    "    print('-' * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
