{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Data Acquisition for Aleksi's pmi-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import json\n",
    "import tqdm\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "import re\n",
    "util_dir = os.path.abspath('../utils')\n",
    "sys.path.append(util_dir)\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = ['jsonzip', 'output', 'corpus']\n",
    "make_dirs(directories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "projects = \"epsd2/admin/ur3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving http://build-oracc.museum.upenn.edu/json/epsd2-admin-ur3.zip as jsonzip/epsd2-admin-ur3.zip.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2348989e7314c8ebf4712d68609b643",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=591486913.0, style=ProgressStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "p = format_project_list(projects)\n",
    "oracc_download(p);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `parsejson()` function below works in a way that is similar to the `parsejson()` functions we discussed in Chapter 2. Each .json file to be parsed represents a single text. The list `l` collects lemmatizations in the format CF\\[GW\\]POS (for instance lugal\\[king\\]N). When the function has gone through the entire file it adds the \"#\" symbol to the end of the list (to indicate the end of a document) and extends the list `lemm_l` with the list `l`. This will create one long list of lemmas, with individual documents separated by the \"#\" symbol.\n",
    "\n",
    "In addition to text breaks, we also want to mark logical and physical breaks within a single document (so that text windows do not jump over such breaks). Logical and physical breaks in the text are marked in the JSON with a `state` node. This node has a restricted vocabulary to indicate breaks, traces, illegible lines, horizontal rulings, etc. The vocabulary that marks a logical or physical break is collected in the list `breakage`. When such a node is encountered, the symbol \"#\" is added to the list `l`. Then the process resumes. \n",
    "\n",
    "The rest of the function takes care of special situations:\n",
    "* Unlemmatized words - words that are damaged or unknown should not be included. They are replaced by an underscore (\"_\"). Such words do not contribute to our analysis, but should also not be removed because we do not want to create artificial neighbors.\n",
    "* Damaged personal names are like unlemmatized words and are replaced by an underscore. Such names are lemmatized as PN with a citation form in the format Lu₂.x.\n",
    "* Numbers are of no interest here and are entirely removed.\n",
    "* Year names are removed. Year names are important for dating, for political history and for understanding the ideology of the period. They do not contribute meaningful collocates to the transactions in the documents studied here.\n",
    "* Words that are not in Sumerian are removed. Note that loans from Akkadian are considered to be Sumerian words, those are retained. Occasionally Ur III texts may include Akkadian prepositions or fully conjugated Akkadian verbs. Such words are removed.\n",
    "\n",
    "The process also skips lemmas that derive from the \"sign\" and \"pronunciation\" columns of lexical lists. That is not relevant in the current context, but may become relevant if you wish to use this code on a wider set of texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemm_l = []\n",
    "l = []\n",
    "ids_ = []\n",
    "breakage = ['illegible', 'traces', 'missing', 'effaced','other', 'blank', 'ruling']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parsejson(text):\n",
    "    for JSONobject in text[\"cdl\"]:\n",
    "        if \"cdl\" in JSONobject: \n",
    "            parsejson(JSONobject)\n",
    "        elif JSONobject.get(\"state\", \"\") in breakage:  # at any logical or physical break\n",
    "            if l:\n",
    "                if not l[-1] == \"#\":\n",
    "                    l.append(\"#\")\n",
    "            continue\n",
    "        elif JSONobject.get(\"subtype\",\"\") in ['sg', 'pr']: # skip the fields \"sign\" and \"pronunciation\"\n",
    "            continue                                     # in lexical texts\n",
    "        elif JSONobject.get(\"subtype\", \"\")[:5] in [\"seal \", \"envel\"]: # seal 1, seal 2, etc. or envelope\n",
    "            if l:\n",
    "                if not l[-1] == \"#\":\n",
    "                    l.append(\"#\")\n",
    "            continue\n",
    "        elif JSONobject.get(\"ftype\", \"\") == \"yn\":\n",
    "            continue # skip year names\n",
    "        elif \"f\" in JSONobject:          # copy all the lemmatization data in the variable word\n",
    "            word = JSONobject[\"f\"]\n",
    "            if word[\"lang\"][:3] != \"sux\": #only Sumerian and Emesal\n",
    "                continue\n",
    "            if word.get(\"pos\", \"\") == \"n\":  # omit numbers\n",
    "                continue\n",
    "            if \"cf\" in word:\n",
    "                #for some reason some words appear without pos. Provisionally treated as Noun\n",
    "                lemm = f\"{word['cf']}[{word['gw']}]{word.get('pos', 'N')}\"  \n",
    "                lemm = lemm.replace(' ', '-') # remove commas and spaces from lemm\n",
    "                lemm = lemm.replace(',', '')\n",
    "            else:\n",
    "                lemm = \"_\" # if word is unlemmatized enter a place holder\n",
    "            if \"x\" in word.get(\"cf\",\"\").lower():  # partly damaged PN; enter placeholder\n",
    "                lemm = \"_\"\n",
    "            l.append(lemm)           # append the lemmatization to the list l\n",
    "    return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4caf9731e1b5483999a514708abe0573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='epsd2/admin/ur3', max=72683.0, style=ProgressStyle(descri…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for project in p:\n",
    "    file = \"jsonzip/\" + project.replace(\"/\", \"-\") + \".zip\"\n",
    "    try:\n",
    "        z = zipfile.ZipFile(file)       # create a Zipfile object\n",
    "    except:\n",
    "        print(file + \" does not exist or is not a proper ZIP file\")\n",
    "        continue\n",
    "    files = z.namelist()     # list of all the files in the ZIP\n",
    "    files = [name for name in files if \"corpusjson\" in name and name[-5:] == '.json']                                                                                                  #that holds all the P, Q, and X numbers.\n",
    "    for filename in tqdm(files, desc = project):                            #iterate over the file names\n",
    "        id_no = filename[-13:-5]\n",
    "        if id_no in ids_ and not \"X\" in id_no: # Check if P/Q number is already in there\n",
    "            continue        # a text may appear in multiple projects\n",
    "        id_text = project + id_no # id_text is, for instance, blms/P414332\n",
    "        ids_.append(id_text)\n",
    "        try:\n",
    "            text = z.read(filename).decode('utf-8')         #read and decode the json file of one particular text\n",
    "            data_json = json.loads(text)                # make it into a json object (essentially a dictionary)\n",
    "            l = parsejson(data_json)\n",
    "            if len(l) > 1:\n",
    "                if not l[-1] == \"#\":\n",
    "                    l.append(\"#\")\n",
    "                lemm_l.extend(l.copy())\n",
    "                l.clear()\n",
    "        except:\n",
    "            print(id_text + ' is not available or not complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above results in the lemm_l, which holds the lemmatizations of all the texts in their original order, one lemma per list entry. Secondly, the list ids_ holds all the text IDs. These IDs are not further used, but were collected to prevent duplication, which may be an issue if you derive data from more than one project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save in \"Word per Line\" format for use in pmi-embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/lemmas.wpl\", \"w\", encoding=\"utf8\") as w:\n",
    "    w.write(\"\\n\".join(lemm_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "emb_dir = os.path.abspath('../../pmi-embeddings/src')\n",
    "sys.path.append(emb_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import make_embeddings as embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"output/lemmas.wpl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunk_size = 400000\n",
    "parameters = {\n",
    "    \"window_size\": 3,\n",
    "    \"min_count\": 1,\n",
    "    \"subsampling_rate\": None,\n",
    "    \"k_factor\": 0,\n",
    "    \"dynamic_window\": True,\n",
    "    \"window_scaling\": False,\n",
    "    \"verbose\": True\n",
    "           }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> ------------------------------------------------\n",
      "> Reading output/lemmas.wpl...\n",
      "   Corpus statistics:\n",
      "      spans       127101\n",
      "      tokens      2244551\n",
      "      types       28572\n",
      "      lacunae     154207\n",
      "      stopwords   0\n",
      "      frag. rate: 0.07\n",
      "    (1.53 seconds)\n",
      "> ------------------------------------------------\n",
      "> Extracting bigrams...\n",
      "> Matrix size: 816359184 (9454 kB)\n",
      "> Non-zero elements: 1181732\n",
      "    (9.17 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "embeddings = embs.Cooc(file_name, chunk_size, **parameters)\n",
    "embeddings.count_cooc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmi_parameters = {\n",
    "    'shift_type': 0,\n",
    "    'alpha': 0.75,\n",
    "    'lambda_': None, \n",
    "    'threshold': 5\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Calculating PMI...\n",
      "> Context distribution smoothing α=0.75\n",
      "    (0.09 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "embeddings.calculate_pmi(**pmi_parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> SVD...\n",
      "> Normalizing vectors...\n",
      "    (14.30 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "dimensions = 300\n",
    "eigenvalue_weighting = 0.0\n",
    "embeddings.factorize(dimensions, eigenvalue_weighting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> Filtering zero-vectors...\n",
      "> Saving 28497 non-zero vectors (75 discarded)... \n",
      "    (9.12 seconds)\n",
      "> ------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "vector_file = 'output/sahala.vec'\n",
    "embeddings.save_vectors(vector_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "13066 + 15506"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"output/sahala.vec\", \"r\", encoding=\"utf8\") as s:\n",
    "    sahala = s.readlines()[1:]\n",
    "with open(\"output/vec_file.txt\", \"r\", encoding = \"utf8\") as c:\n",
    "    compass = c.readlines()[1:]\n",
    "lemmas_s = set()\n",
    "lemmas_c = set()\n",
    "for line in sahala:\n",
    "    lemma = line.split()[0]\n",
    "    lemmas_s.add(lemma)\n",
    "for line in compass:\n",
    "    lemma = line.split()[0]\n",
    "    lemmas_c.add(lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmas_s), len(lemmas_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirig = lemmas_s - lemmas_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
